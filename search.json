[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Note",
    "section": "",
    "text": "序言\n本書謹收錄平時我上課的筆記，在本書中，我盡量少用些數學符號來解釋時間序列的公式，比起直接使用公式，我更傾向直觀的理解一個概念。\n本書將使用 python 時做時間序列的相關資料，你可以在每個時間序列的概念下看到簡易的 python 代碼，基本上，除了特別麻煩的公式，我都直接提供最原始的程式碼，這樣比較助於我們理解底層的運算邏輯，至於視覺化方面，我採用了 python 套件 plotly 來完成，由於他的底層運算是基於 d3.js，因此可以做到使用者互動，這也是我選擇 plotly 的原因"
  },
  {
    "objectID": "ts_intro.html#time-series-v.s.-cross-sectional",
    "href": "ts_intro.html#time-series-v.s.-cross-sectional",
    "title": "1  什麼是時間序列",
    "section": "1.1 Time Series v.s. Cross Sectional",
    "text": "1.1 Time Series v.s. Cross Sectional\n最常被混淆的概念就是時間序列 (time series) 與橫斷面 (cross sectional) 資料，他們的差別如下\n\nTime Series v.s. Cross Sectional\n\n\n\n\n\n\n\n差別\nTime Series\nCross Sectional\n\n\n\n\n定義\n固定時間點蒐集的序列資料\n同一時間點不同個體的資料\n\n\n範例\n2000/01/01 到 2020/01/01 台積電每日股票的收盤價\n2000/01/01 台灣前 50 大上市公司的股價"
  },
  {
    "objectID": "ts_intro.html#為什麼我們需要時間序列模型",
    "href": "ts_intro.html#為什麼我們需要時間序列模型",
    "title": "1  什麼是時間序列",
    "section": "1.2 為什麼我們需要時間序列模型",
    "text": "1.2 為什麼我們需要時間序列模型\n接續前文的定義，假設我們想調查 2000/01/01 到 2020/01/01 台積電每日股票的收盤價，每天收盤價對於我們而言都是一個隨機變數，而且我們不能保證這些隨機變數屬於同一個分配，因為他們來自不同時間點，換句話說，每天的收盤價是從不同母體抽樣出來的，而且只有一個樣本點，當只有一個樣本點時，就不能採用過往我們熟悉的統計工具解決問題，因此，我們勢必要假設資料與資料之間存在某種的關係，而這個關係是會隨著時間變化的"
  },
  {
    "objectID": "ts_intro.html#數學上的時間序列",
    "href": "ts_intro.html#數學上的時間序列",
    "title": "1  什麼是時間序列",
    "section": "1.3 數學上的時間序列",
    "text": "1.3 數學上的時間序列\n我們假設隨機變數\n\n\\({U}_t, t\\in \\{1,\\cdots,T\\}\\)\n\n表示時間序列的隨機變數。我們可以把 \\(U_t\\) 看做是隨著時間變化的變數，例如每日股價，每年人口增長率等等，總結來說 \\(U_t\\) 包含兩個面向\n\n隨機變數 \\(U \\to\\) 表示隨著時間變化我們感興趣的指標\n時間區間 \\(t \\to\\) 表示時間的過程，通常為等量區間，例如每天、每年\n\n至此，我們已經大致了解時間序列是什麼樣的資料了，但是，事實上並不是這麼簡單，我們常常看到的時間序列表現形式通常會長的像下面這樣\n\\(U_t=T_t+S_t+C_t+R_t\\)\n其中\n\n\\(T_t:\\) 趨勢效應 (Trend component)\n\\(S_t:\\) 季節效應 (Seasonal component)\n\\(C_t:\\) 循環效應 (Cyclical component)\n\\(R_t:\\) 隨機效應 (Random component)\n\n時間序列資料大概可以分解成上面的四個主要部分，接下來我們來一一解析他們\n\n1.3.1 趨勢效應\n我們常常說一檔股票有上升趨勢或下降趨勢，其實很容易理解，趨勢表示長期時間來看，資料平移變化的方向。例如在下面的例子中，我們調查一般民眾獲取新聞的方式，可以很明顯看到\n\nDownward Trend: Television、Newspaper\nNo Trend: Radio\nUpward Trand: Internet\n\n表示隨著時間發展，使用網路獲取信息的人們越來越多了\n\n\nCode\ntitle = 'Main Source for News'\nlabels = ['Television', 'Newspaper', 'Internet', 'Radio']\ncolors = ['rgb(67,67,67)', 'rgb(115,115,115)', 'rgb(49,130,189)', 'rgb(189,189,189)']\n\nmode_size = [8, 8, 12, 8]\nline_size = [2, 2, 4, 2]\n\nx_data = np.vstack((np.arange(2001, 2014),)*4)\n\ny_data = np.array([\n    [74, 82, 80, 74, 73, 72, 74, 70, 70, 66, 66, 69],\n    [45, 42, 50, 46, 36, 36, 34, 35, 32, 31, 31, 28],\n    [13, 14, 20, 24, 20, 24, 24, 40, 35, 41, 43, 50],\n    [18, 21, 18, 21, 16, 14, 13, 18, 17, 16, 19, 23],\n])\n\nfig = go.Figure()\n\nfor i in range(0, 4):\n    fig.add_trace(go.Scatter(x=x_data[i], y=y_data[i], mode='lines',\n        name=labels[i],\n        line=dict(color=colors[i], width=line_size[i]),\n        connectgaps=True,\n    ))\n\n    # endpoints\n    fig.add_trace(go.Scatter(\n        x=[x_data[i][0], x_data[i][-1]],\n        y=[y_data[i][0], y_data[i][-1]],\n        mode='markers',\n        marker=dict(color=colors[i], size=mode_size[i])\n    ))\n\nfig.update_layout(\n    xaxis=dict(\n        showline=True,\n        showgrid=False,\n        showticklabels=True,\n        linecolor='rgb(204, 204, 204)',\n        linewidth=2,\n        ticks='outside',\n        tickfont=dict(\n            family='Arial',\n            size=12,\n            color='rgb(82, 82, 82)',\n        ),\n    ),\n    yaxis=dict(\n        showgrid=False,\n        zeroline=False,\n        showline=False,\n        showticklabels=False,\n    ),\n    autosize=False,\n    margin=dict(\n        autoexpand=False,\n        l=100,\n        r=20,\n        t=110,\n    ),\n    showlegend=False,\n    plot_bgcolor='white'\n)\n\nannotations = []\n\n# Adding labels\nfor y_trace, label, color in zip(y_data, labels, colors):\n    # labeling the left_side of the plot\n    annotations.append(dict(xref='paper', x=0.05, y=y_trace[0],\n                                  xanchor='right', yanchor='middle',\n                                  text=label + ' {}%'.format(y_trace[0]),\n                                  font=dict(family='Arial',\n                                            size=16),\n                                  showarrow=False))\n    # labeling the right_side of the plot\n    annotations.append(dict(xref='paper', x=0.95, y=y_trace[11],\n                                  xanchor='left', yanchor='middle',\n                                  text='{}%'.format(y_trace[11]),\n                                  font=dict(family='Arial',\n                                            size=16),\n                                  showarrow=False))\n# Title\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Main Source for News',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\n# Source\nannotations.append(dict(xref='paper', yref='paper', x=0.5, y=-0.1,\n                              xanchor='center', yanchor='top',\n                              text='Source: PewResearch Center & ' +\n                                   'Storytelling with data',\n                              font=dict(family='Arial',\n                                        size=12,\n                                        color='rgb(150,150,150)'),\n                              showarrow=False))\n\nfig.update_layout(annotations=annotations)\n\nfig.show()\n\n\n\n                                                \n\n\n\n\n1.3.2 季節效應與循環效應\n季節效應與週期效應都是週期性產生的影響，但大家較易於混淆這兩者的概念，簡單來說，如果頻率變化固定且已知，則為季節性，例如氣溫的變化會隨著四季更迭，降雨量也會根據不同月份而不同，若頻率變化不固定，則為循環性\n例如下面這個範例有週期性的特徵，且每個同樣的時間區間就會有相同的頻率，則為季節性\n\n\nCode\ntitle = 'Seasonal Effect'\n\nx_data = np.linspace(-10, 10, 100)\n\ny_data = np.array(\n    np.sin(x_data) + np.random.normal(size=x_data.shape) * 0.25\n)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines+markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n下面這張圖雖然具有週期性特徵，但是其頻率不固定，雖然固定的一個時間區間會產生高峰，但是高峰會隨著時間越來越高\n\n\nCode\ntitle = 'Cyclical Effect'\n\nx_data = np.linspace(-10, 10, 100)\n\ny_data = np.array(\n    np.sin(x_data)*np.exp(x_data/10) + np.random.normal(size=x_data.shape) * 0.25\n)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines+markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n\n\n1.3.3 隨機效應\n最後一個為隨機效應，通常表示資料的隨機性，也最好理解，因為她就是隨著時間產生的一個隨機效應\n例如下面這張圖，雖然我們是使用 sin 函數去生成的，但是我們給他加上一個常態分配的隨機抽樣，整體就變得不是那麼明顯了\n\n\nCode\ntitle = 'Seasonal Effect'\n\nx_data = np.linspace(-10, 10, 100)\n\ny_data = np.array(\n    np.sin(x_data) + np.random.normal(size=x_data.shape) * 1\n)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines+markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()"
  },
  {
    "objectID": "ts_intro.html#我怎麼知道資料是否適合時間序列分析",
    "href": "ts_intro.html#我怎麼知道資料是否適合時間序列分析",
    "title": "1  什麼是時間序列",
    "section": "1.4 我怎麼知道資料是否適合時間序列分析",
    "text": "1.4 我怎麼知道資料是否適合時間序列分析\n如何知道資料屬於哪一種效應是一種非常仰賴經驗判斷的方法，例如從下面這張圖來說，我們幾乎無法斷定資料是否存在某種效應，最多也只能說是沒有一個明顯的趨勢效應\n\n\nCode\ntitle = 'Cyclical Effect'\n\nx_data = np.linspace(-10, 10, 1000)\n\ny_data = np.array(\n    np.sin(x_data)*np.exp(x_data/100)/10 + np.random.normal(size=x_data.shape)\n)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n從最初到現在，我們也可以看出，我們對於資料存在著多種假設，我們基於這些假設去判斷資料最可能的模型，然而，當有一些模型假設錯誤時，很長造成模型擬合效果不彰，因此，對於資料使用時間序列模型是最後的手段，最好還是先嘗試其他模型\n讓我們回到上面的那筆資料，當我們暫時忽略時間的效應，按照資料的隨機變數畫出下面的分布圖時，我們會發現它其實很像是一個常態分配，但好像又比常態分配寬一點，這讓我們懷疑到底時間有沒有存在影響，又或是他的影響到底多大。所以接下來，我們來講講怎麼看待這件事情\n\nimport plotly.figure_factory as ff\nimport numpy as np\n\nhist_data = [y_data]\n\ngroup_labels = ['Data (no T)']\ncolors = ['#A56CC1']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot(hist_data, group_labels, colors=colors,\n                         bin_size=.2, show_rug=False)\n\n# Add title\nfig.update_layout(title_text='Hist and Curve Plot')\nfig.show()"
  },
  {
    "objectID": "ts_intro.html#平穩性-stationary",
    "href": "ts_intro.html#平穩性-stationary",
    "title": "1  什麼是時間序列",
    "section": "1.5 平穩性 Stationary",
    "text": "1.5 平穩性 Stationary\n在時間序列分析中，我們必須對於資料有簡單又合理的假設，才可以容易建立模型，平穩性就是其中一個，簡單來說，平穩性表示你的資料還是有時間效應，但是這個效應影響不大，其主要分為兩種\n\n1.5.1 弱平穩 Weak stationary\n我們先來看看弱平穩的數學定義\n若 \\(\\{Z_t\\}\\) 被稱為弱 (second-order or covariance stationary) 平穩過程，則\n\n\\(\\mu(t)=C:\\) \\(\\mu\\) 與時間無關\n\\(\\gamma(t, t-k)=\\gamma(0, k):\\) 共變異函數只與滯後 (lag) 有關\n\n簡單而言，平均變化與時間無關，且變數之間的相關性只在滯後上有關係。例如，白噪聲就是一個弱平穩過程，因為白噪聲 \\(\\sim \\mathcal{N}(0,\\sigma^2I)\\)。隨機遊走是非平穩的，例如下面我們建立一個例子\n\\[\nX_t = X_{t-1} + \\epsilon\n\\]\n其中 \\(\\epsilon\\) 為白噪聲，則我們有\n\n\\(\\mathbb{E}(X_t)=\\mathbb{E}(X_{t-1})\\)\n\\(\\mbox{Var}(X_t)=t\\sigma^2\\)\n\n\n\nCode\ntitle = 'Seasonal Effect'\n\nT = 1000\nwalks = []\n\nloc = 0\nfor i in range(T):\n    loc += + np.random.normal(0,1)\n    walks.append(loc)\nwalks = np.array(walks)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=walks, mode='lines',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n因為變異數與時間有關，因此不是弱平穩，但是當我們去檢驗差分 \\(D_t=X_t-X_{t-1}\\) 的分布時，他又是弱平穩的，因為\n\n\\(\\mathbb{E}(D_t)=0\\)\n\\(\\mbox{Var}(X_t)=\\sigma^2\\)\n\n下面我們畫圖來直觀感受一下差分的魅力\n\n\nCode\ntitle = 'Seasonal Effect'\n\nD = walks[1:] - walks[:-1]\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T-1)), y=D, mode='lines',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n是不是完全變成白噪聲了啊，所以我們會發現，很多時間序列的方法都使用差分的技巧來將原先的資料分解到平穩狀態\n\n\n1.5.2 強平穩 Strict stationary\n接著，我們介紹另外一種平穩過程，強平穩，他的數學定義如下\n若 \\(\\{Z_t\\}\\) 被稱為強平穩過程，則\n\\(Z_{t_1}, Z_{t_2}, \\cdots, Z_{t_n}\\) 的聯合分配與 \\(Z_{t_1-k}, Z_{t_2-k}, \\cdots, Z_{t_n-k}\\) 的聯合分配相同\n講白話一點，就是我們選一個時間序列的某一段切下去，跟前面的一段時間完全相同，注意，這裡我們並沒有限制時間長度以及滯後，因此，這個假設強到很難出現，所以實務上還是使用弱平穩居多\nNote:\n\n弱平穩與強平穩之間並未存在包含關係\n當聯合分配屬於多元常態時，弱平穩與強平穩等價\n\n\n\n1.5.3 Sample AutoCorrelation Function (ACF)\n到這裡，我們已經講完了平穩過程的簡易概念，注意到在弱平穩過程中，我們提到了變數之間的時間關係只與滯後有關，那麼滯後的關係要怎麼衡量呢? 假設我們想知道某一時間與前面時間的相關性，例如在隨機遊走的範例中，當前變數與前一時間點的變數有關，當我們得知這一訊息時，就可以****使用差分的技巧將資料分解為弱平穩**。ACF 就可以幫我們做到這一目的\n首先，自相關的定義如下\n\\[\n\\rho_{\\ell}=\\frac{\\mathrm{cov}(r_{t},r_{t-\\ell})}{\\mathrm{var}(r_{t})}\n\\]\n其中\n\n自相關係數假設資料存在弱平穩\n\\(\\gamma_{k}=\\operatorname{Cov}(r_{t},r_{t-k})=E[(r_{t}-\\mu)(r_{t-k}-\\mu)]:\\) 表示滯後 \\(k\\) 的自變異\n\\(\\rho_0=1:\\) 自己與自己的相關性為 1\n\\(\\rho_k=\\rho_{-k}:\\) 表示變數之間的關係只與滯後長度有關\n\n說白了，自相關係數是個衡量變數 Lag-\\(\\ell\\) 的相關性方法，上面公式看起來很完美，但是 \\(\\mu\\) 基本上是不知道的 (我們不可能知道真實的模型)，因此，實務上我們使用經驗自相關函數\n\\[\n\\hat{\\rho}_{\\ell}=\\frac{\\Sigma_{t=1}^{T-\\ell}(r_{t}-\\bar{r})(r_{t+\\ell}-\\bar{r})}{\\Sigma_{t=1}^{T}(r_{t}-\\bar{r})^{2}}\n\\]\n其中\n\n\\(\\bar r:\\) 樣本平均\n\\(T:\\) 樣本數量\n\n\n1.5.3.1 為什麼要假設弱平穩\n想像一下，如果我們沒有弱平穩，那麼每一個時間點的分配都不相同 (平均與變異數都是時間的函數)，這時候假設我們的資料總共有 \\(T\\) 個時間，那麼就等同於我們有 \\(T\\) 個母體，每一個母體都只有一個樣本，那麼這樣幾乎無法進行相關係數的運算。又或者，對於一條沒有平穩假設的時間序列，相關係數的運算除了基於時間差之外，也會基於時間點，但是 ACF 的精神就是想知道時間差的相關，因此，我們會假設平均與變異數與時間無關\n\n\n1.5.3.2 Python 實現\npython 實現很非常方便，我們可以透過 numpy.correlate 計算，numpy.correlate(a, v, mode) 將 v 作為 filter 掃過 a，類似訊號中的捲機運算，初始時我們會將 v 滾動與 a 對其進行捲機運算 (若大小不一則補零)，而後逐次向右移動。在官網的資訊中，numpy.correlate 提供了三種模式\n\\[\nc_k=\\sum_n a_{n+k} * \\bar v_n\n\\]\n其中\n\n\\(\\bar v_n\\) 表示 complex conjugation\nmode: 捲機運算方式\n\nvalid: v 直接與 a 內積，直到最後一項與 a 最後一項對齊\nsame: v 最後一項由左至右內積，直到最後一項與 a 最後一項對齊\nfull: v 最後一項由左至右內積，直到第一項與 a 最後一項對齊\n\n\n\ndef acf(series, lag=None):\n    \"\"\"\n    Calculate the autocorrelation function (ACF) for a given time series.\n\n    Parameters:\n        series (array-like): The time series data.\n        lag (int, optional): The maximum lag for which to calculate the ACF.\n            If None (default), the ACF is calculated for all possible lags.\n\n    Returns:\n        acf_values (array): Autocorrelation values for the given time series.\n            If `lag` is provided, returns a single value representing\n            autocorrelation at that lag. Otherwise, returns an array of\n            autocorrelation values for all lags.\n    \"\"\"\n    series = np.asarray(series)\n    n = len(series)\n\n    mean = np.mean(series)\n    centered_data = series - mean\n\n    # Calculate autocovariance function\n    acov = np.correlate(centered_data, centered_data, mode='full')\n    acf_values = acov / sum(centered_data ** 2)\n\n    if lag is not None:\n        return acf_values[(n-1):(n-1+lag)]\n    else:\n        return acf_values[(n-1):]\n\n接下來我們比較隨機遊走和一階差分的 ACF 圖表\n\n\nCode\nrw_acf = acf(walks, lag=90)\ntitle = 'ACF Random Walks'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=rw_acf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nd_acf = acf(D, lag=90)\ntitle = 'ACF D'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T-1)), y=d_acf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n可以看到在差分完成之後，資料中就基本不存在什麼相關性了\n另外，我們也可以透過 python 套件 statsmodels.graphics.tsaplots.plot_acf 直接完成，如下\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplot_acf(walks, lags=90)\nplt.show()\n\nplot_acf(D, lags=90)\nplt.show()"
  },
  {
    "objectID": "ts_model.html#auto-regressive-ar-模型",
    "href": "ts_model.html#auto-regressive-ar-模型",
    "title": "2  時間序列模型",
    "section": "2.1 Auto Regressive (AR) 模型",
    "text": "2.1 Auto Regressive (AR) 模型\n接下來，我們介紹時間序列中最簡單的模型，AR 模型。AR 模型假設當前資訊只與前一時間資訊有關，公式如下\n\\[\nr_t=\\phi_0+\\phi_1r_{t-1}+a_t\n\\]\n其中\n\n\\(\\phi_0:\\) 截距項\n\\(\\phi_1:\\) 前一時間的資訊保留程度\n\n這個模型假設其實自動包含弱穩態的要求，我們可以檢查他的平均數和變異數\n\\[\n{\\mathbb E}(r_{t}|r_{t-1})=\\phi_{0}+\\phi_{1}r_{t-1},\\qquad\\mathrm{Var}(r_{t}|r_{t-1})=\\mathrm{Var}(a_{t})=\\sigma_{a}^{2}\n\\]\n我們可以看到當我們給定前一時間的資訊後，平均與變異數都與時間無關了，因此自動符合若穩態的定義\nNote:\n\nMarkov 性質: 如果細心一點會發現，當前時間只與前一時間有關是馬可夫鏈的性質，因此，馬可夫鏈保證給定當前時間，下一期的時間分配完全確定，也就是不需要再往前看更多期時間\n\n\n2.1.1 AR(p) 模型\n前面我們假設想要知道當前的資訊，我們只需要檢查前一期即可，我們把這種模型稱為 AR(1) 模型，同理，我們可以定義 AR(p) 模型\n\\[\nr_t=\\phi_0+\\phi_1r_{t-1} + \\cdots+\\phi_pr_{t-p} + a_t\n\\]\n也就是當前時間與前面 p 期時間都有關\n\n\n2.1.2 什麼時候該使用 AR 模型\n接下來我們會想知道到底什麼時後該使用 AR 模型，最簡單的方法其實是使用 ACF 圖判斷，那麼 AR(p) 的 ACF 圖長什麼樣子呢? 我們來畫畫看\n\nimport numpy as np\nimport plotly.graph_objects as go\n\ndef acf(series, lag=None):\n    \"\"\"\n    Calculate the autocorrelation function (ACF) for a given time series.\n\n    Parameters:\n        series (array-like): The time series data.\n        lag (int, optional): The maximum lag for which to calculate the ACF.\n            If None (default), the ACF is calculated for all possible lags.\n\n    Returns:\n        acf_values (array): Autocorrelation values for the given time series.\n            If `lag` is provided, returns a single value representing\n            autocorrelation at that lag. Otherwise, returns an array of\n            autocorrelation values for all lags.\n    \"\"\"\n    series = np.asarray(series)\n    n = len(series)\n\n    mean = np.mean(series)\n    centered_data = series - mean\n\n    # Calculate autocovariance function\n    acov = np.correlate(centered_data, centered_data, mode='full')\n    acf_values = acov / sum(centered_data ** 2)\n\n    if lag is not None:\n        return acf_values[(n-1):(n-1+lag)]\n    else:\n        return acf_values[(n-1):]\n\n同上週，我們先定義一個 ACF 計算函數，接下來簡單生成 AR(1) 過程的資料，我們就拿隨機遊走做示範\n\n\nCode\ntitle = 'Random Walks'\n\nT = 1000\nwalks = []\n\nloc = 0\nfor i in range(T):\n    loc = loc + np.random.normal(0,1)\n    walks.append(loc)\nwalks = np.array(walks)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=walks, mode='lines',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n接下來我們畫出 ACF\n\n\nCode\nrw_acf = acf(walks, lag=90)\ntitle = 'ACF Random Walks'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=rw_acf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n看起來好像很平穩，但是我們調整隨機遊走前面的係數呢? 我們將係數調整為 0.3，也就是\n\\[\nr_t=0.3*r_{t-1}+a_t\n\\]\n我們在重複上面的過程，我們有\n\n\nCode\ntitle = 'Random Walks (coef=0.3)'\n\nT = 1000\nwalks = []\n\nloc = 0\nfor i in range(T):\n    loc = .7 * loc + np.random.normal(0,1)\n    walks.append(loc)\nwalks = np.array(walks)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=walks, mode='lines',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nrw_acf = acf(walks, lag=90)\ntitle = 'ACF Random Walks (coef=0.3)'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=rw_acf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n可以看到 ACF 前幾項似乎呈現指數分配速度的遞減，那麼他們到底有什麼關係呢? 接下來我們就用數學的方法來看看係數與 ACF 之間的關係\n\n\n2.1.3 AR(1) 平均數\n，在回答這個問題之前，我們先來看看 AR 模型的性質，這裡我們用 \\(\\gamma_j\\) 表達相差 \\(j\\) 期的共變異數，我們有\n\\[\n\\begin{aligned}\nr_t&=\\phi_0+\\phi_1r_{t-1}+a_t \\\\\n\\mathbb{E}(r_t)&=\\phi_0+\\phi_1\\mathbb{E}(r_{t-1}) \\\\\n\\mu&=\\phi_0+\\phi_1\\mu \\\\\n\\mu&=\\frac{\\phi_0}{1-\\phi_1}\n\\end{aligned}\n\\]\n其中\n\n\\(\\mathbb{E}(r_t)=\\mathbb{E}(r_{t-1})=\\mu\\)\n\\(\\phi_0=0 \\to E(r_t)=0\\)，因此，對於一個平穩的 AR(1) 序列，\\(\\phi_0\\) 與資料的時間的平均數有關\n\n且根據上面的性質，我們可以寫出\n\\[\n\\begin{aligned}\nr_{t}-\\mu&=\\phi_{1}(r_{t-1}-\\mu)+a_{t} \\\\\nr_{t}-\\mu&=a_{t}+\\phi_{1}a_{t-1}+\\phi_{1}^{2}a_{t-2}+\\cdots\\\\ {{\\phantom{\\infty}}}\\\\\n&=\\sum_{i=0}^{\\infty}\\phi_{1}^{i}a_{t-i}\n\\end{aligned}\n\\]\n是不是很神奇呀，其實 AR 模型就是我們最初時間線性模型的其中一種\n\n\n2.1.4 AR(1) 變異數\n\\[\n\\begin{aligned}\nr_t&=\\phi_0+\\phi_1r_{t-1}+a_t \\\\\n\\mathrm{Var}(r_t)&=\\phi_1^2\\mathrm{Var}(r_{t-1})+\\sigma_a^2 \\\\\n\\mathrm{Var}(r_t)&=\\frac{\\sigma^2_a}{1-\\phi_1^2}\n\\end{aligned}\n\\]\n其中\n\n\\(\\mathrm{Cov}(r_{t-1}, a_t)=0:\\) 因為 \\(a_t\\) 為白噪聲序列，我們將 \\(r_{t-1}\\) 寫成 \\(\\phi_0+\\phi_1r_{t-2}+a_{t-1}\\) ，因此兩者沒有關係\n\\(\\phi_1^2 &lt; 1:\\) 因為隨機變數的變異數是非負且有界的\n\n且根據上述的公式我們可以推出\n\nAR(1) 模型是弱穩態的充分必要條件是 \\(|\\phi_1| &lt; 1\\)\n\n\n\n2.1.5 AR(1) 自相關係數\n自相關係數 \\(\\phi_\\ell=\\gamma_\\ell/\\gamma_0\\)，我們在先前的章節中已經計算過 \\(\\gamma_0\\) (變異數)，因此我們只要知道 \\(\\gamma_\\ell=\\mathbb{E}[(r_{t}-\\mu)(r_{t-\\ell}-\\mu)]\\) 即可，這裡直接給出結論\n\\[\n\\gamma_{\\ell} = \\begin{cases}\n\\phi_{1}\\gamma_{1} + \\sigma_{a}^{2} & \\text{if } \\ell = 0, \\\\\n\\phi_{1}\\gamma_{\\ell-1} & \\text{if } \\ell &gt; 0.\n\\end{cases}\n\\]\n因此我們有\n\n\\(\\gamma_0=\\mathrm{Var}(r_t)=\\frac{\\sigma^2_a}{1-\\phi_1^2}\\)\n\\(\\gamma_\\ell=\\phi_1\\gamma_{\\ell-1}\\) (\\(\\ell &gt; 0\\))\n\n把上面的第二個公式慢慢迭代就有\n\\[\n\\rho_{\\ell}=\\phi_{1}\\rho_{\\ell-1}=\\cdots=\\phi_{1}^{\\ell}\\rho_{0},\\quad\\mathrm{for}\\quad\\ell\\ge0\n\\]\n因此我們得到一個結論\n\nAR(1) 模型的 ACF 圖呈現指數遞減\n\n相同的手法可以用在 AR(2) 或其他，但是這裡我們就不提供證明過程了\n\n\n2.1.6 AR Order 估計\n那我們要用什麼方法估計資料到底是 AR 多少的模型呢? 如果是資深機器學習或資料科學家，他們應該會說使用交叉驗證來找到最好的 p，當然沒錯，但是我們必須注意到在時間序列的資料中是有順序性的，統計學家們較為常用的方法是使用 AIC、BIC 選模型，簡單來講，就是跑一個迴圈去檢查每一個 p，然後選出最好的 BIC 和 AIC，我們這裡簡介 AIC 和 BIC 的使用時機\n\nAIC: 收斂速度很快，可以有效保證收斂到真實值附近，但是較難趨近真值\nBIC: 需要大樣本，在大樣本下，BIC 可以收斂到真值，但是速度較慢\n\n因此假設我們的資料是 AR(3) 模型，AIC 可能很快就可以收斂到 2~4 之間，但是比較難到達 3，但是 BIC 不一樣，你只要給足夠多的資料，他就會趨近 3\n除了使用 AIC 和 BIC 之外，我們也可以使用 Partial Autocorrelation Function (PACF)，在介紹 PACF 之前，我們先想一個問題，假設我想要調查時間差為 K 期的相關性，我當然可以直接計算 lag-K 的自相關係數，但是，我要如何保證這 K 期的相關性並不是 K 個 1 期時間差的影響組合成的? PACF 就試圖解決這個問題\n\\[\n\\begin{array}{l}\n{{r_{t}=\\phi_{0,1}+\\phi_{1,1}r_{t-1}+e_{1t},}}\\\\\n{{r_{t}=\\phi_{0,2}+\\phi_{1,2}r_{t-1}+\\phi_{2,2}r_{t-2}+e_{2t},}}\\\\\n{{r_{t}=\\phi_{0,3}+\\phi_{1,3}r_{t-1}+\\phi_{2,3}r_{t-2}+\\phi_{3,3}r_{t-3}+e_{3t},}}\\\\\n{{\\vdots}}\\quad \\quad {{\\vdots}}\n\\end{array}\n\\]\n其中\n\n\\(\\phi_{i,j}\\) 表示第 \\(\\text{AR}(j)\\) 模型 \\(r_{t-i}\\) 的影響係數\n\\(e\\) 為誤差項\n\\(\\phi_0\\) 為常數項\n\n我們最感興趣的就是 \\(\\hat \\phi_{j,j}\\) (lag-\\(i\\) PACF)，\\(\\hat \\phi_{2,2}\\) 表示添加 \\(r_{t-2}\\) 後比起 AR(1) 模型增加的資訊量。有很多方法都可以求解 PACF，包含 OLS, Yule-Walker, Burg”s method, Levinson-Durbin 等等，以下提供 python 代碼\n\n# source https://www.jianshu.com/p/811f9ea0b52d\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef yule_walker(ts, order):\n    ''' \n    Solve yule walker equation\n    '''\n    x = np.array(ts) - np.mean(ts)\n    n = x.shape[0]\n\n    r = np.zeros(order+1, np.float64) # to store acf\n    r[0] = x.dot(x) / n # r(0)\n    for k in range(1, order+1):\n        r[k] = x[:-k].dot(x[k:]) / (n - k) # r(k)\n\n    R = toeplitz(r[:-1])\n\n    return np.linalg.solve(R, r[1:]) # solve `Rb = r` to get `b`\n\n\ndef pacf(ts, k):\n    ''' \n    Compute partial autocorrelation coefficients for given time series，unbiased\n    '''\n    res = [1.]\n    for i in range(1, k+1):\n        res.append(yule_walker(ts, i)[-1])\n    return np.array(res)\n\n我們同樣使用隨機遊走的範例說明，可以看到除了 lag-1，其他相關性全部被壓到 0 附近\n\n\nCode\nrw_acf = pacf(walks, k=90)\ntitle = 'PACF Random Walks (coef=0.3)'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=rw_acf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n這裡我們就可以談談 PACF 的特性\n\n\\(\\hat \\phi_{j,j} \\to \\phi_p \\quad \\text{as} \\quad T \\to \\infty\\)\n\\(\\hat \\phi_{\\ell, \\ell} \\to 0 \\quad \\forall \\ell &gt; j\\)\n\\(\\hat \\phi_{\\ell, \\ell}\\) 的漸進變異數是 \\(1/T\\) 對所有 \\(\\ell &gt; j\\)\n\n表示對於一個 AR(p) 序列，PACF 在 lag-p 會有明顯的截斷面\n\n\n2.1.7 AR 模型參數估計\n最後，我們來講講怎麼估計 AR(p) 模型，AR(p) 模型可以看成是一個多元線性回歸，只是變數都是過去時間，因此我們有\n\\[\nr_{t}=\\phi_{0}+\\phi_{1}r_{t-1}+\\dots+\\phi_{p}r_{t-p}+a_{t},\\quad t=p+1,\\dots,\\,T\n\\]\n當我們把這 \\(T-p\\) 條公式寫開之後，就可以使用 OLS 或者梯度下降方式去估算參數\n\\[\n\\hat{r}_{t}=\\hat{\\phi}_{0}+\\hat{\\phi}_{1}r_{t-1}+\\cdot\\cdot\\cdot+\\hat{\\phi}_{p}r_{t-p}\n\\]\n有了估計式之後，我們就可以計算誤差 (殘差項) \\({\\hat{a}}_{t}=r_{t}-{\\hat{r}}_{t}\\)\nNote: 如果序列資料來自於 AR(p) 模型，那麼該模型的參數應該呈現出白噪聲的樣子\n\n\n2.1.8 AR 模型的預測\n當我們建立一個模型之後，當然是想用他去預測未來的資料，這個概念在時間序列中稱為 forecasting，如果寫的數學一點就像\n\\[\nE\\{[r_{h+\\ell}-\\hat{r}_{h}(\\ell)]^{2}|F_{h}\\}\\leq\\operatorname*{min}_{g}E[(r_{h+\\ell}-g)^{2}|F_{h}]\n\\]\n其中\n\n\\(F_h\\) 表示在 \\(h\\) 時間點之前我們所擁有的所有資訊\n\n簡單來說，就是站在時間點 \\(h\\)，當我們想要估計 \\(l\\) 期時間後的值，我們會找一個 MSE 最小的 \\(g\\) 最為我們的預測值\n舉例來說，我們想要用一個 AR(p) 模型估計 \\(l\\) 天後的股價，我們當然可以寫\n\\[\nr_{h+\\ell}=\\phi_{0}+\\phi_{1}r_{h+\\ell-1}+\\cdot\\cdot\\cdot+\\phi_{p}r_{h+\\ell-p}+a_{h+\\ell}\n\\]\n但是要注意到，我們事實上完全沒有 \\(1,2,\\cdots,l-1\\) 天的股價資訊，而這些資訊又是 AR(p) 模型必要的特徵，因此，我們只能使用類似於滑動窗口的方式，先估計一天後，兩天後，最後才到 \\(l\\) 天後，這過程，我們必須不斷使用我們的估計值去取代真實值，因此距離現在的時間越遠，我們的估計就越不準\n\n\n2.1.9 AR 練習\n\n首先我們先來生成一組 AR(3) 的 資料\n\n\\[\nr_t=0.01+0.1*r_{t-1}-0.1*r_{t-3}+a_t\n\\]\n\nnp.random.seed(42) \n\ndef generate_ar_data(n, mu=0, sigma=1):\n    \"\"\"\n    Generate AR data with the specified equation.\n    \n    Parameters:\n        n (int): Number of data points to generate.\n        mu (float): Mean of the random noise.\n        sigma (float): Standard deviation of the random noise.\n        \n    Returns:\n        np.array: Array of AR data points.\n    \"\"\"\n    # Initialize arrays\n    ar_data = np.zeros(n)\n    epsilon = np.random.normal(mu, sigma, n)\n    \n    # Generate AR data\n    for t in range(3, n):\n        ar_data[t] = 0.01 + 0.1 * ar_data[t-1] - 0.1 * ar_data[t-3] + epsilon[t]\n    \n    return ar_data\ndata = generate_ar_data(500)\n\ntitle = \"Simulated AR(3) data\"\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=data, mode='lines',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n                                                \n\n\n\n畫出 ACF plot\n\n\n\nCode\ndata_acf = acf(data, lag=10)\ntitle = 'ACF AR(3)'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=data_acf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n可以看到在 lag-3 的相關係數最大\n\n畫出 PCAF plot\n\n\n\nCode\ndata_pacf = pacf(data, k=10)\ntitle = 'PACF AR(3)'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=data_pacf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n同 ACF，可以看到在 lag-3 的相關係數最大\n\n用 AIC 估計 AR order\n\n\nfrom statsmodels.tsa.ar_model import AutoReg\n\nbest = float(\"inf\")\nbest_order = -1\nfor order in range(1, 10):\n    ar_model = AutoReg(data, lags=order).fit()\n    print(f\"AR({order}) AIC: \", ar_model.aic)\n    if ar_model.aic &lt; best:\n        best = ar_model.aic\n        best_order = order\nprint(\"Selected best order: \", best_order)\n\nAR(1) AIC:  1407.3539983067183\nAR(2) AIC:  1407.2913499349963\nAR(3) AIC:  1402.828223161296\nAR(4) AIC:  1398.1094017600367\nAR(5) AIC:  1397.0403557874909\nAR(6) AIC:  1396.8567941625547\nAR(7) AIC:  1393.7280505207527\nAR(8) AIC:  1389.2987596911696\nAR(9) AIC:  1389.2034251860591\nSelected best order:  9\n\n\n\n用最好 AIC 選出來的 order 重新 fit 一次 data\n\n\nar_model = AutoReg(data, lags=best_order).fit()\nprint(ar_model.summary())\n\n                            AutoReg Model Results                             \n==============================================================================\nDep. Variable:                      y   No. Observations:                  500\nModel:                     AutoReg(9)   Log Likelihood                -683.602\nMethod:               Conditional MLE   S.D. of innovations              0.974\nDate:                Mon, 04 Mar 2024   AIC                           1389.203\nTime:                        13:05:55   BIC                           1435.364\nSample:                             9   HQIC                          1407.331\n                                  500                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0117      0.044      0.267      0.790      -0.075       0.098\ny.L1           0.0928      0.045      2.053      0.040       0.004       0.181\ny.L2           0.0003      0.045      0.007      0.994      -0.088       0.089\ny.L3          -0.0903      0.045     -2.003      0.045      -0.179      -0.002\ny.L4          -0.0882      0.045     -1.947      0.052      -0.177       0.001\ny.L5           0.0406      0.045      0.894      0.372      -0.048       0.130\ny.L6           0.0287      0.045      0.635      0.526      -0.060       0.117\ny.L7          -0.0319      0.045     -0.708      0.479      -0.120       0.057\ny.L8          -0.0871      0.045     -1.929      0.054      -0.176       0.001\ny.L9          -0.0058      0.045     -0.127      0.899      -0.094       0.083\n                                    Roots                                    \n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1            1.2163           -0.5437j            1.3323           -0.0669\nAR.2            1.2163           +0.5437j            1.3323            0.0669\nAR.3            0.5383           -1.1338j            1.2551           -0.1794\nAR.4            0.5383           +1.1338j            1.2551            0.1794\nAR.5           -1.3244           -0.5728j            1.4430           -0.4350\nAR.6           -1.3244           +0.5728j            1.4430            0.4350\nAR.7           -0.6288           -1.2763j            1.4228           -0.3229\nAR.8           -0.6288           +1.2763j            1.4228            0.3229\nAR.9          -14.7478           -0.0000j           14.7478           -0.5000\n-----------------------------------------------------------------------------\n\n\n\n檢查殘差是否為白噪聲\n\n\n\nCode\ndata_acf = acf(ar_model.resid, lag=10)\ntitle = 'ACF AR(3)'\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=list(range(T)), y=data_acf, mode='markers',\n    name=\"sin\",\n    line=dict(color=\"blue\", width=1),\n    connectgaps=True,\n))\nfig.update_layout(title=title)\nfig.show()\n\n\n\n                                                \n\n\n可以發現在 fit 完 AR 模型之後，殘差的 ACF 圖中相關性已然趨近於 0 點附近，表示殘差之間並沒有相關性"
  },
  {
    "objectID": "ts_model.html#moving-average-ma-模型",
    "href": "ts_model.html#moving-average-ma-模型",
    "title": "2  時間序列模型",
    "section": "2.2 Moving Average (MA) 模型",
    "text": "2.2 Moving Average (MA) 模型\n我們前面談到 AR 模型都是有限 order 的，也就是 p 是有限的，但是當我會感興趣，當 order 無限時會發生什麼，也就是\n\\[\nr_t=\\phi_0 + a_t + \\sum_{i=1}^\\infty\\phi_ir_{t-i}\n\\]\n雖然我們可以往回頭看無限遠的過去，但是這是不切實際的，因為同時也有無限多個參數需要估計，因此，我們勢必要對參數做一些假設，例如 \\(\\phi_i=-\\theta_1^{i}\\)，因此我們有\n\\[\nr_t=\\phi_0 + a_t + \\sum_{i=1}^\\infty-\\theta_1^{i}r_{t-i}\n\\]\n在上面的公式中，要求 \\(|\\theta_i| &lt; 1\\)，才能夠保證距離當前越遠的時間影響越小，而加上負號只是為了計算方便，有趣的地方來了，當我們根據上面的公式計算 \\(r_t-\\theta_1r_{t-1}\\) 時我們有\n\\[\nr_t=\\phi_0(1-\\theta_1)+a_t-\\theta_1a_{t-1}\n\\]\n可以看到 MA(1) 模型是將當前 shock \\(a_t\\) 和前一期 shock 做加權平均。有了這個公式之後，我們就可以開始介紹更為廣義的 MA 模型\n\\[\nr_t = c_0 + a_t + \\sum_{i=1}^q \\theta_ia_{t-i}\n\\]\n其中\n\n\\(c_0\\) 常數，用來代表平均\n\\(a_i\\) 為過去 shock (誤差)\n\nMA 模型假設時間序列資料的波動變異都來自於 shocks，因此 MA 模型利用當前與過往的 shocks 做加權平均，也可以想 \\(r_t\\) 是當前 shocks 加上過往的預測誤差的加權平均。\nNote:\n雖然上述公式看起來像是回歸模型，但是 \\(a_t\\) 是無法觀測到的\n\n2.2.1 MA(q) 平均數\n因為 shocks 是平均值為 0 的白噪聲序列，因此 MA(q) 的平均數非常簡單可以寫成\n\\[\n\\mathbb{E}(r_t) = c_0\n\\]\n這個公式自帶滿足弱穩態的平均值與時間無關條件，接下來我們檢查變異數\n\n\n2.2.2 MA(q) 變異數\n同樣地，因為 \\(a_i\\) 與 \\(a_j\\) 沒有相關性 \\((i\\neq j)\\)，我們也可以計算變異數如下\n\\[\n\\mathrm{Var}(r_{t})=(1+\\theta_{1}^{2}+\\theta_{2}^{2}+\\cdots+\\theta_{q}^{2})\\sigma_{a}^{2}\n\\]\n從上面看出，MA 也滿足變異數與時間無關之條件，因此，MA 序列是保證弱穩態的\n\n\n2.2.3 MA(q) 自相關係數\n這裡我們從 MA(1) ACF 出發，來討論 MA(q) 的自相關係數應該長什麼樣子，以下我們假設 \\(c_0=0\\)\n\\[\nr_{t-\\ell}r_{t}\\,=\\,r_{t-\\ell}a_{t}\\,+\\,\\theta_{1}r_{t-\\ell}a_{t-1}\n\\]\n對上式取期望值以及代入 \\(\\mathrm{Var}(r_{t})=(1+\\theta_{1}^{2})\\sigma_{a}^{2}\\) 之後，我們有\n\\[\n\\rho_{0}=1,\\quad\\rho_{1}=\\frac{\\theta_{1}}{1+\\theta_{1}^{2}},\\quad\\rho_{\\ell}=0,\\quad\\mathrm{for}\\quad\\ell&gt; 1\n\\]\n從上面的自相關係數可以發現，在時間差大於一期之後相關係數全為 0，表示 MA(1) 會在 Lag-1 之後突然全部降為 0，由此，我們可以推測 MA(q) 模型 ACF 的特性為 \\(\\rho_ell=0\\) 只要 \\(\\ell&gt;q\\)，因此，當我們看到 ACF 圖中在 lag-q 之後有明顯的截斷，就可以推斷有可能是 MA(q) 模型\nNote:\n因為 MA(q) 公式表明時間差 &gt; q 期完全無關，因此 MA 是一個有限記憶的模型"
  }
]